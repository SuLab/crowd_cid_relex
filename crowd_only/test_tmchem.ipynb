{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the chemical highlighting abilities of tmChem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2015-06-16 Tong Shu Li<br>\n",
    "Last updated: 2015-08-17 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our crowdsourcing approach relies upon being able to exhaustively annotate all chemical annotations in the original raw text. Here we test to see how well tmChem can annotate chemicals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/toby/Code/util\")\n",
    "from file_util import read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.data_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first take the data for biocreative V and strip it down to the original raw text:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"data/tmchem/tmchem_training.txt\", \"w\") as out:\n",
    "    for line in read_file(\"data/training/CDR_TrainingSet.txt\"):\n",
    "        if len(line) == 0:\n",
    "            out.write(\"\\n\")\n",
    "        elif len(line) > 0 and \"|\" in line:\n",
    "            vals = line.split(\"|\")\n",
    "            if vals[1] in [\"t\", \"a\"]:\n",
    "                out.write(\"{0}\\n\".format(\"|\".join(vals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Development data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"data/tmchem/tmchem_development.txt\", \"w\") as out:\n",
    "    for line in read_file(\"data/development/CDR_DevelopmentSet.txt\"):\n",
    "        if len(line) == 0:\n",
    "            out.write(\"\\n\")\n",
    "        elif len(line) > 0 and \"|\" in line:\n",
    "            vals = line.split(\"|\")\n",
    "            if vals[1] in [\"t\", \"a\"]:\n",
    "                out.write(\"{0}\\n\".format(\"|\".join(vals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run tmChem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% mv data/tmchem/tmchem_*.txt ~/Code/tmChem/tmChem.M2.ver02/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/toby/Code/tmChem/tmChem.M2.ver02\n"
     ]
    }
   ],
   "source": [
    "% cd ~/Code/tmChem/tmChem.M2.ver02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/toby/Code/tmChem/tmChem.M2.ver02'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input format: PubTator\n",
      "Running tmChem on 500 docs in tmchem_development.txt ... Finished in 61 seconds. \n",
      "Input format: PubTator\n",
      "Running tmChem on 500 docs in tmchem_training.txt ... Finished in 63 seconds. \n"
     ]
    }
   ],
   "source": [
    "! perl tmChem.pl -i input -o output Model/All.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% mv output/*.tmChem ~/Research/Projects/biocreativeV/crowdbefree/crowd_only/data/tmchem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/toby/Research/Projects/biocreativeV/crowdbefree/crowd_only\n"
     ]
    }
   ],
   "source": [
    "% cd ~/Research/Projects/biocreativeV/crowdbefree/crowd_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/toby/Research/Projects/biocreativeV/crowdbefree/crowd_only'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab tmChem's output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmchem_training = parse_input(\"data/tmchem\", \"tmchem_training.txt.tmChem\", is_gold = False, return_format = \"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmchem_development = parse_input(\"data/tmchem\", \"tmchem_development.txt.tmChem\", is_gold = False, return_format = \"list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab the gold standard data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gold_training = parse_input(\"data/training\", \"CDR_TrainingSet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gold_development = parse_input(\"data/development\", \"CDR_DevelopmentSet.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acronym resolver\n",
    "\n",
    "If an abbreviation for a chemical is found in parentheses immediately after a term tmChem has identified as a chemical, then replace the id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def acronym_resolver(dataset):\n",
    "    \"\"\"\n",
    "    Given a parsed dataset, trys to resolve acronyms.\n",
    "    \"\"\"\n",
    "    for paper in dataset:\n",
    "        used = [False] * len(paper.annotations)\n",
    "\n",
    "        full_text = \"{0} {1}\".format(paper.title, paper.abstract)\n",
    "\n",
    "        M = len(paper.annotations)\n",
    "\n",
    "        changes = 0\n",
    "        for i, definition in enumerate(paper.annotations[:-1]):\n",
    "            if not used[i] and definition.has_mesh:\n",
    "                next_annot = paper.annotations[i+1]\n",
    "\n",
    "                if (next_annot.start == 2 + definition.stop\n",
    "                    and full_text[next_annot.start - 1] == \"(\"\n",
    "                    and full_text[next_annot.stop] == \")\"):\n",
    "\n",
    "                    # found an acronym definition\n",
    "#                     print \"pmid\", paper.pmid\n",
    "#                     print \"Found an acronym definition using:\"\n",
    "#                     print definition\n",
    "\n",
    "\n",
    "                    used[i] = True\n",
    "\n",
    "                    for j, annot in enumerate(islice(paper.annotations, i+1, None)):\n",
    "                        if annot.text == next_annot.text and annot.uid != definition.uid:\n",
    "#                             print \"Changing annotation to definition:\"\n",
    "#                             print annot\n",
    "#                             print\n",
    "                            annot.uid = definition.uid\n",
    "                            used[i + 1 + j] = True\n",
    "\n",
    "                            changes += 1\n",
    "\n",
    "    #                     print\n",
    "\n",
    "\n",
    "        if changes > 0:\n",
    "            print changes\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_copy = copy.deepcopy(tmchem_training)\n",
    "changed_training = acronym_resolver(training_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "9\n",
      "5\n",
      "4\n",
      "9\n",
      "8\n",
      "8\n",
      "4\n",
      "4\n",
      "21\n",
      "5\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "5\n",
      "2\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "5\n",
      "3\n",
      "2\n",
      "6\n",
      "2\n",
      "2\n",
      "10\n",
      "9\n",
      "10\n",
      "6\n",
      "5\n",
      "1\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "7\n",
      "1\n",
      "1\n",
      "2\n",
      "15\n",
      "6\n",
      "10\n",
      "2\n",
      "1\n",
      "4\n",
      "5\n",
      "4\n",
      "9\n",
      "4\n",
      "3\n",
      "5\n",
      "12\n",
      "3\n",
      "6\n",
      "2\n",
      "4\n",
      "9\n",
      "10\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "dev_copy = copy.deepcopy(tmchem_development)\n",
    "changed_dev = acronym_resolver(dev_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the performance of tmChem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def results(program_output, gold_data):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    sum_chemicals = 0\n",
    "    \n",
    "    stuff = []\n",
    "    for prog_data, gold_std in zip(program_output, gold_data):\n",
    "        assert prog_data.pmid == gold_std.pmid\n",
    "        \n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for prog_annot in prog_data.annotations:\n",
    "            if prog_annot.stype == \"chemical\":\n",
    "                for gold_annot in gold_std.annotations:\n",
    "                    if prog_annot.stype == \"chemical\":\n",
    "                        if (prog_annot.text == gold_annot.text\n",
    "                            and prog_annot.start == gold_annot.start\n",
    "                            and prog_annot.stop == gold_annot.stop):\n",
    "                            \n",
    "                            #assert prog_annot.uid == gold_annot.uid, \"{0} {1}\".format(prog_annot, gold_annot)\n",
    "                            # treat non mesh differently\n",
    "                            \n",
    "                            p = set(filter(lambda v: v.uid_type == \"MESH\", prog_annot.uid))\n",
    "                            g = set(filter(lambda v: v.uid_type == \"MESH\", gold_annot.uid))\n",
    "                            \n",
    "                            if p == g:\n",
    "                                tp += 1\n",
    "                            else:\n",
    "                                fp += 1\n",
    "                                stuff.append(prog_annot.text)\n",
    "                               \n",
    "                            \n",
    "        for gold_annot in gold_std.annotations:\n",
    "            if gold_annot.stype == \"chemical\":\n",
    "                sum_chemicals += 1\n",
    "                \n",
    "        TP += tp\n",
    "        FP += fp\n",
    "        \n",
    "    recall = TP / sum_chemicals\n",
    "    precision = TP / (TP + FP)\n",
    "    \n",
    "    f_score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    print \"F score:\", f_score\n",
    "\n",
    "    print \"recall: {0}\".format(TP / sum_chemicals)\n",
    "    print \"precision: {0}\".format(TP / (TP + FP))\n",
    "\n",
    "    print \"TP: {0}\".format(TP)\n",
    "    print \"FP: {0}\".format(FP)\n",
    "    print \"all gold annotations: {0}\".format(sum_chemicals)\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F score: 0.99014111734\n",
      "recall: 0.984432058428\n",
      "precision: 0.995916780089\n",
      "TP: 5122\n",
      "FP: 21\n",
      "all gold annotations: 5203\n"
     ]
    }
   ],
   "source": [
    "stuff = results(tmchem_training, gold_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MFL',\n",
       " 'MFL regimen',\n",
       " 'MFL regimen',\n",
       " 'MFL regimen',\n",
       " 'CPA',\n",
       " 'CPA',\n",
       " 'CPA',\n",
       " 'CPA',\n",
       " 'CPA',\n",
       " 'CPA',\n",
       " 'alendronate',\n",
       " 'alendronate sodium',\n",
       " 'alendronate',\n",
       " 'alkylating agents',\n",
       " 'alkylating agents',\n",
       " 'alkylating agents',\n",
       " 'alkylating agents',\n",
       " 'PO2',\n",
       " 'DA',\n",
       " 'HVA',\n",
       " 'DA']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F score: 0.877668077104\n",
      "recall: 0.872825883673\n",
      "precision: 0.88256429652\n",
      "TP: 4667\n",
      "FP: 621\n",
      "all gold annotations: 5347\n"
     ]
    }
   ],
   "source": [
    "stuff = results(tmchem_development, gold_development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F score: 0.99014111734\n",
      "recall: 0.984432058428\n",
      "precision: 0.995916780089\n",
      "TP: 5122\n",
      "FP: 21\n",
      "all gold annotations: 5203\n"
     ]
    }
   ],
   "source": [
    "kek = results(changed_training, gold_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F score: 0.935213916314\n",
      "recall: 0.93005423602\n",
      "precision: 0.940431164902\n",
      "TP: 4973\n",
      "FP: 315\n",
      "all gold annotations: 5347\n"
     ]
    }
   ],
   "source": [
    "kek = results(changed_dev, gold_development)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The acronym resolver makes a 0.05 improvement (!!!) in the F-score for the dev set! Include in the final!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PMID 12615818, for tmChem's output, CPA the abbreviation is a different id from \"cyproterone acetate\". How to fix this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def results(program_output, gold_std_data):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    sum_chemicals = 0\n",
    "    for p_data, gold_std in zip(program_output, gold_std_data):\n",
    "        assert p_data.pmid == gold_std.pmid\n",
    "\n",
    "        # check tmChem's output against the gold standard\n",
    "\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        for annot in p_data.chemicals:\n",
    "            if annot in gold_std.chemicals:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "\n",
    "        sum_chemicals += len(gold_std.chemicals)\n",
    "        TP += tp\n",
    "        FP += fp\n",
    "        \n",
    "    recall = TP / sum_chemicals\n",
    "    precision = TP / (TP + FP)\n",
    "    \n",
    "    f_score = 2 * precision * recall / (precision + recall)\n",
    "    \n",
    "    print \"F score:\", f_score\n",
    "\n",
    "    print \"recall: {0}\".format(TP / sum_chemicals)\n",
    "    print \"precision: {0}\".format(TP / (TP + FP))\n",
    "\n",
    "    print \"TP: {0}\".format(TP)\n",
    "    print \"FP: {0}\".format(FP)\n",
    "    print \"all gold annotations: {0}\".format(sum_chemicals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F score: 0.98931416359\n",
      "recall: 0.985294117647\n",
      "precision: 0.993367147874\n",
      "TP: 5092\n",
      "FP: 34\n",
      "all gold annotations: 5168\n"
     ]
    }
   ],
   "source": [
    "results(tmchem_training, gold_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F score: 0.871685893544\n",
      "recall: 0.812028657617\n",
      "precision: 0.940803844474\n",
      "TP: 4307\n",
      "FP: 271\n",
      "all gold annotations: 5304\n"
     ]
    }
   ],
   "source": [
    "results(tmchem_development, gold_development)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In conclusion, it looks like tmChem does pretty well at identifying the chemicals in a piece of text. The recall is a lot lower on the development set, but is still high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we really were worried about recall, then we could always add more concept recognizers to drive up total recall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
