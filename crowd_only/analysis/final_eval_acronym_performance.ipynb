{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the acronym resolution filter's effect on NER performance\n",
    "\n",
    "Tong Shu Li<br>\n",
    "Created on: 2015-12-03<br>\n",
    "Last updated: 2015-12-03<br>\n",
    "\n",
    "Here we test the effect of the acronym resolution filter we used for the official evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_model import parse_input\n",
    "from src.lingpipe.file_util import save_file\n",
    "from src.eval_perf import performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the PMID mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paper_mapping = save_file(\"testset_mapping.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = os.path.abspath(os.path.join(\"..\", \"data\", \"gold_standard\"))\n",
    "fname = \"CDR_TestSet.txt\"\n",
    "\n",
    "eval_gold = parse_input(loc, fname, is_gold = True, return_format = \"dict\", fix_acronyms = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the NER processed testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loc = os.path.abspath(os.path.join(\"..\", \"data\", \"final_eval\"))\n",
    "fname = \"CDR_annotated_testset.txt\"\n",
    "\n",
    "no_fix = parse_input(loc, fname, is_gold = False, return_format = \"dict\", fix_acronyms = False)\n",
    "fixed = parse_input(loc, fname, is_gold = False, return_format = \"dict\", fix_acronyms = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_pmids(papers):\n",
    "    ans = {}\n",
    "    for pmid, paper in papers.items():\n",
    "        paper.pmid = paper_mapping[pmid]\n",
    "        ans[paper_mapping[pmid]] = paper\n",
    "        \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_fix = fix_pmids(no_fix)\n",
    "fixed = fix_pmids(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_concepts(papers, stype):\n",
    "    assert stype in [\"chemical\", \"disease\"]\n",
    "    ans = set()\n",
    "    for pmid, paper in papers.items():\n",
    "        for annot in paper.annotations:\n",
    "            if stype == annot.stype:\n",
    "                uids = [ont_id.flat_repr for ont_id in annot.uid]\n",
    "                uids = \"|\".join(sorted(uids))\n",
    "                \n",
    "                ans.add((pmid, stype, annot.start, annot.stop, annot.text, uids))\n",
    "            \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_chem = get_concepts(eval_gold, \"chemical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5385"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval_chem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_fix_chem = get_concepts(no_fix, \"chemical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5052"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(no_fix_chem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_chem = get_concepts(fixed, \"chemical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5052"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fixed_chem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# True pos: 4250\n",
      "# False pos: 802\n",
      "# False neg: 1135\n",
      "Precision: 0.8412509897070467\n",
      "Recall: 0.7892293407613742\n",
      "F-score: 0.8144102711507137\n"
     ]
    }
   ],
   "source": [
    "performance(eval_chem, no_fix_chem, human_readable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# True pos: 4460\n",
      "# False pos: 592\n",
      "# False neg: 925\n",
      "Precision: 0.882818685669042\n",
      "Recall: 0.8282265552460538\n",
      "F-score: 0.8546517198428667\n"
     ]
    }
   ],
   "source": [
    "performance(eval_chem, fixed_chem, human_readable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_dise = get_concepts(eval_gold, \"disease\")\n",
    "no_fix_dise = get_concepts(no_fix, \"disease\")\n",
    "fixed_dise = get_concepts(fixed, \"disease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# True pos: 3224\n",
      "# False pos: 1053\n",
      "# False neg: 1200\n",
      "Precision: 0.7537993920972644\n",
      "Recall: 0.72875226039783\n",
      "F-score: 0.7410642454890243\n"
     ]
    }
   ],
   "source": [
    "performance(eval_dise, no_fix_dise, human_readable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# True pos: 3224\n",
      "# False pos: 1053\n",
      "# False neg: 1200\n",
      "Precision: 0.7537993920972644\n",
      "Recall: 0.72875226039783\n",
      "F-score: 0.7410642454890243\n"
     ]
    }
   ],
   "source": [
    "performance(eval_dise, fixed_dise, human_readable = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
